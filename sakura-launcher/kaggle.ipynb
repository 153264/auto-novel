{"cells":[{"cell_type":"markdown","metadata":{},"source":["kaggle 挂机的时间未到，翻译器到一半显示 TypeError: Failed to fetch，重试还是不行可能是 ngrok 额度用完了，请登录 ngrok 官网检查顶栏是否有提示。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-23T03:13:45.929898Z","iopub.status.busy":"2024-04-23T03:13:45.929147Z","iopub.status.idle":"2024-04-23T03:20:16.745536Z","shell.execute_reply":"2024-04-23T03:20:16.744258Z","shell.execute_reply.started":"2024-04-23T03:13:45.929864Z"},"trusted":true},"outputs":[],"source":["%cd -q /kaggle/working\n","!rm -rf ./llama.cpp\n","\n","!echo 准备编译llama.cpp...\n","!git clone -q -c advice.detachedHead=false -b b2755 --depth 1 https://github.com/ggerganov/llama.cpp.git\n","!cp -r /usr/local/cuda-12.1/targets /usr/local/nvidia/\n","\n","!echo 开始编译llama.cpp...\n","%cd -q /kaggle/working/llama.cpp/\n","!make LLAMA_CUDA=1 CUDA_PATH=/usr/local/nvidia server -j$(nproc) -s\n","!ls -lh ./server\n","\n","!echo 配置python环境...\n","!pip install -q pyngrok\n","\n","!echo 编译完成"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-23T03:24:16.594634Z","iopub.status.busy":"2024-04-23T03:24:16.594224Z"},"trusted":true},"outputs":[],"source":["NGROK_TOKEN = \"\"\n","REPO = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9-GGUF\"\n","MODEL = \"sakura-14b-qwen2beta-v0.9-iq4_xs_ver2\"\n","DOUBLE = False\n","\n","model_dir = \"/kaggle/working/llama.cpp/models/\"\n","\n","\n","def main():\n","    ports = [\"8080\", \"8081\"] if DOUBLE else [\"8080\"]\n","\n","    setup_ngrok(ports)\n","    download_model(\n","        repo_id=REPO,\n","        filename=f\"{MODEL}.gguf\",\n","        local_dir=model_dir,\n","    )\n","\n","    from multiprocessing import Pool\n","\n","    pool = Pool(processes=len(ports))\n","    pool.map(run_server, enumerate(ports))\n","\n","\n","def setup_ngrok(ports):\n","    from pyngrok import conf, ngrok\n","\n","    print(\"设置Ngrok\")\n","    conf.get_default().auth_token = NGROK_TOKEN\n","    conf.get_default().monitor_thread = False\n","\n","    port_to_url = {}\n","\n","    ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n","    for ssh_tunnel in ssh_tunnels:\n","        port = ssh_tunnel.config[\"addr\"].removeprefix(\"http://localhost:\")\n","        if port in ports:\n","            port_to_url[port] = ssh_tunnel.public_url\n","\n","    for port in ports:\n","        if port in port_to_url:\n","            url = port_to_url[port]\n","        else:\n","            ssh_tunnel = ngrok.connect(addr=port)\n","            url = ssh_tunnel.public_url\n","        print(f\"隧道{port}： {url}\")\n","    print()\n","\n","\n","def download_model(repo_id, filename, local_dir):\n","    from huggingface_hub import hf_hub_download\n","    from huggingface_hub.utils import (\n","        RepositoryNotFoundError,\n","        EntryNotFoundError,\n","        LocalEntryNotFoundError,\n","    )\n","\n","    print(f\"开始下载模型：{repo_id}/{filename}\")\n","    try:\n","        hf_hub_download(\n","            repo_id=repo_id,\n","            filename=filename,\n","            local_dir=local_dir,\n","        )\n","    except RepositoryNotFoundError:\n","        print(\"模型下载错误：无法找到要下载的仓库，请检查 REPO 参数。\")\n","        exit(0)\n","    except LocalEntryNotFoundError:\n","        print(\"模型下载错误：网络已禁用或者无法连接。\")\n","        exit(0)\n","    except EntryNotFoundError:\n","        print(\"模型下载错误：无法找到要下载的模型，请检查 MODEL 参数。\")\n","        exit(0)\n","    else:\n","        print(\"模型下载成功\")\n","    print()\n","\n","\n","def run_server(param):\n","    import os\n","    import subprocess\n","\n","    pos, port = param\n","    p = subprocess.Popen(\n","        [\n","            \"/kaggle/working/llama.cpp/server\",\n","            \"-m\",\n","            f\"{model_dir}/{MODEL}.gguf\",\n","            \"-ngl\",\n","            \"99\",\n","            \"-c\",\n","            \"2048\",\n","            \"-a\",\n","            MODEL,\n","            \"--port\",\n","            port,\n","        ],\n","        env={**os.environ, \"CUDA_VISIBLE_DEVICES\": str(pos)},\n","        stdout=subprocess.PIPE,\n","        stderr=subprocess.STDOUT,\n","        encoding=\"utf8\",\n","        bufsize=0,\n","    )\n","    while True:\n","        for line in p.stdout:\n","            if line.startswith(\"{\"):\n","                message = format_message(line)\n","                print(f\"{pos}-{port} {message}\")\n","            else:\n","                print(f\"{pos}-{port} {line}\", end=\"\")\n","\n","\n","def format_message(line):\n","    import json\n","    from datetime import datetime\n","\n","    job = json.loads(line)\n","    timestamp = datetime.fromtimestamp(job[\"timestamp\"])\n","    msg = None\n","\n","    if job[\"function\"] == \"print_timings\":\n","        msg = (\n","            job[\"msg\"]\n","            .replace(\"tokens per second\", \"token/s\")\n","            .replace(\"ms per token\", \"ms/token\")\n","        )\n","    else:\n","        for key_to_delete in [\"tid\", \"timestamp\", \"level\", \"function\", \"line\"]:\n","            del job[key_to_delete]\n","\n","        if \"msg\" in job:\n","            msg = job[\"msg\"]\n","            del job[\"msg\"]\n","\n","            if job:\n","                msg = f\"{msg}: {json.dumps(job)}\"\n","\n","    if msg is None:\n","        msg = line.strip()\n","    return f\"{timestamp.strftime('%H:%M:%S')} {msg}\"\n","\n","\n","main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
